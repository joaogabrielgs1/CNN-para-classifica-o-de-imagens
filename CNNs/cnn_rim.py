# -*- coding: utf-8 -*-
"""rim.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KdSRkd5pFAVfNtgf0O9dxTHeGAKSAOWi
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import os
import random
import shutil
import matplotlib.pyplot as plt
import cv2

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import image_dataset_from_directory
from keras.preprocessing.image import ImageDataGenerator

train_directory = '/content/drive/MyDrive/Datasets/rim/augmented train'
test_directory = '/content/drive/MyDrive/Datasets/rim/test'
val_directory = '/content/drive/MyDrive/Datasets/rim/val'

categories = os.listdir(train_directory)
print(str(len(categories)),'CATEGORIES are ', categories)

category_count = len(categories)



def preprocess_image(image):
    if image.shape[-1] == 3:
        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray_image = image
    return gray_image


general_datagen = ImageDataGenerator(
    rescale=1./255,
    preprocessing_function=preprocess_image
)
train_generator = general_datagen.flow_from_directory(
    train_directory,
    target_size=(224, 224),
    batch_size=32,
    color_mode='grayscale'
)

valid_generator = general_datagen.flow_from_directory(
    val_directory,
    target_size=(224, 224),
    batch_size=32,
    color_mode='grayscale'
)
test_generator = general_datagen.flow_from_directory(
    test_directory,
    target_size=(224, 224),
    batch_size=32,
    color_mode='grayscale',
    shuffle=False
)

diretorio = "/content/drive/MyDrive/Datasets/rim/augmented train/kidney tumor"

# Lista de todas as imagens no diretório
imagens = os.listdir(diretorio)

# Calcula o número de imagens a serem excluídas (metade das imagens)
numero_de_imagens_para_excluir = len(imagens)
print(numero_de_imagens_para_excluir)

train_groups = len(train_generator)
valid_groups = len(valid_generator)
test_groups = len(test_generator)
print(f"Train groups: {train_groups}")
print(f"Validation groups: {valid_groups}")
print(f"Test groups: {test_groups}")

def conv_layer(inputs, filters, kernel_size=3, padding="valid"):
    x = layers.Conv2D(filters = filters, kernel_size = kernel_size, padding = padding, use_bias = False)(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    return x

def pooling_layer(inputs, pool_size = 2, dropout_rate=0.5):
    x = layers.MaxPooling2D(pool_size = pool_size)(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(dropout_rate)(x)
    return x

def dense_layer(inputs, out, dropout_rate = 0.5):
    x = layers.Dense(out)(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    x = layers.Dropout(dropout_rate)(x)
    return x

keras.backend.clear_session()
inputs = keras.Input(shape = (224, 224, 1))
x = conv_layer(inputs, 64, padding = "same")
x = conv_layer(x, 64)
x = pooling_layer(x)
x = conv_layer(x, 64, padding = "same")
x = conv_layer(x, 64)
x = pooling_layer(x)
x = conv_layer(x, 64, padding = "same")
x = conv_layer(x, 64)
x = pooling_layer(x)
x = conv_layer(x, 64, padding = "same")

x = layers.Flatten()(x)
x = dense_layer(x, 96)

outputs = layers.Dense(category_count, activation = "softmax")(x)
base_model = keras.Model(inputs, outputs)
base_model.summary()

base_model.compile(optimizer =keras.optimizers.Adam(learning_rate=0.001),
               loss = 'categorical_crossentropy',
               metrics = ['accuracy'])
history = base_model.fit(
    train_generator,
    steps_per_epoch = train_groups,
    epochs = 20,
    validation_data = valid_generator,
    validation_steps = valid_groups,
    verbose = 1,
    callbacks=[keras.callbacks.EarlyStopping(monitor='val_accuracy', patience = 10, restore_best_weights = True),
               keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.7, patience = 2, verbose = 1),
    keras.callbacks.ModelCheckpoint(
            filepath = "/content/drive/MyDrive/Datasets/rim/model_rim.h5",
            save_best_only = True,
            monitor = "val_loss")
    ])



accuracy = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]

loss = history.history["loss"]
val_loss = history.history["val_loss"]


print(accuracy[19])
#epochs = range(1, len(accuracy) + 1)

#plt.plot(epochs, accuracy, "bo", label = "Trianing accuracy")
#plt.plot(epochs, val_accuracy, "b-", label = "Validation accuracy")
#plt.title("Accuracy on training and validation data")
#plt.legend()
#plt.figure()

#plt.plot(epochs, loss, "bo", label = "Trianing loss")
#plt.plot(epochs, val_loss, "b-", label = "Validation loss")
#plt.title("loss on training and validation data")
#plt.title("loss on training and validation data")
#plt.legend()
#plt.show()

model = keras.models.load_model("/content/drive/MyDrive/Datasets/rim/model_rim.h5")

test_results = model.evaluate(test_generator)

loss, accuracy = test_results

print(f'Perda (Loss): {loss}')
print(f'Acurácia: {accuracy}')

predictions = model.predict(test_generator)

predicted_classes = np.argmax(predictions, axis=1)

true_classes = test_generator.classes

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(true_classes, predicted_classes)
print(f'Precisão: {accuracy}')

from sklearn.metrics import f1_score
predictions = model.predict(test_generator)
predicted_classes = np.argmax(predictions, axis=1)

true_classes = test_generator.classes

f1 = f1_score(true_classes, predicted_classes, average='weighted')

print(f'F1-Score: {f1}')

from sklearn.metrics import confusion_matrix
predictions = model.predict(test_generator)
predicted_classes = np.argmax(predictions, axis=1)

true_classes = test_generator.classes

confusion = confusion_matrix(true_classes, predicted_classes)

print('Matriz de Confusão:')
print(confusion)