# -*- coding: utf-8 -*-
"""cnn_carro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cv0puDrB0nRSQ8Gy5-qdarOnTEKK5DvG
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import os
import random
import shutil
import matplotlib.pyplot as plt
import cv2

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import image_dataset_from_directory
from keras.preprocessing.image import ImageDataGenerator

train_directory = '/content/drive/MyDrive/Datasets/carro pronto/train'
test_directory = '/content/drive/MyDrive/Datasets/carro pronto/test'
val_directory = '/content/drive/MyDrive/Datasets/carro pronto/val'

categories = os.listdir(train_directory)
print(str(len(categories)),'CATEGORIES are ', categories)

category_count = len(categories)

augmented_gen = ImageDataGenerator(
    rescale=1./255)

general_datagen = ImageDataGenerator(rescale = 1./255)

train_generator = general_datagen.flow_from_directory(
    train_directory,
    target_size = (250, 250),
    batch_size = 16
)
valid_generator = general_datagen.flow_from_directory(
    val_directory,
    target_size = (250, 250),
    batch_size = 16
)
test_generator = general_datagen.flow_from_directory(
    test_directory,
    target_size = (250, 250),
    batch_size = 16,
    shuffle=False
)

train_groups = len(train_generator)
valid_groups = len(valid_generator)
test_groups = len(test_generator)
print(f"Train groups: {train_groups}")
print(f"Validation groups: {valid_groups}")
print(f"Test groups: {test_groups}")

def conv_layer(inputs, filters, kernel_size=3, padding="valid"):
    x = layers.Conv2D(filters = filters, kernel_size = kernel_size, padding = padding, use_bias = False)(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)

    return x

def pooling_layer(inputs, pool_size = 2, dropout_rate=0.5):
    x = layers.MaxPooling2D(pool_size = pool_size)(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(dropout_rate)(x)

    return x

def dense_layer(inputs, out, dropout_rate = 0.5):
    x = layers.Dense(out)(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    x = layers.Dropout(dropout_rate)(x)

    return x

keras.backend.clear_session()
inputs = keras.Input(shape = (250, 250, 3))
x = conv_layer(inputs, 64, padding = "same")
x = conv_layer(x, 64)
x = pooling_layer(x)
x = conv_layer(x, 64, padding = "same")
x = conv_layer(x, 64)
x = pooling_layer(x)
x = conv_layer(x, 64, padding = "same")
x = conv_layer(x, 64)
x = pooling_layer(x)
x = conv_layer(x, 64, padding = "same")
x = conv_layer(x, 64)
x = pooling_layer(x)
x = conv_layer(x, 64, padding = "same")

x = layers.Flatten()(x)
x = dense_layer(x, 128)

outputs = layers.Dense(category_count, activation = "softmax")(x)
base_model = keras.Model(inputs, outputs)
base_model.summary()

base_model.compile(optimizer =keras.optimizers.Adam(learning_rate=0.001),
               loss = 'categorical_crossentropy',
               metrics = ['accuracy'])
#fit model
history = base_model.fit(
    train_generator,
    steps_per_epoch = train_groups,
    epochs = 20, # adding more epochs will increase the acc like 1% or 2%
    validation_data = valid_generator,
    validation_steps = valid_groups,
    verbose = 1,
    callbacks=[keras.callbacks.EarlyStopping(monitor='val_accuracy', patience = 7, restore_best_weights = True),
               keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.7, patience = 7, verbose = 1),
    keras.callbacks.ModelCheckpoint(
            filepath = "/content/drive/MyDrive/Datasets/carro pronto/model_carro.h5",
            save_best_only = True,
            monitor = "val_loss")
    ])

model = keras.models.load_model("/content/drive/MyDrive/Datasets/carro pronto/model_carro.h5")
test_results = model.evaluate(test_generator)
loss, accuracy = test_results
print(f'Perda (Loss): {loss}')
print(f'Acurácia: {accuracy}')

predictions = model.predict(test_generator)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = test_generator.classes
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(true_classes, predicted_classes)
print(f'Precisão: {accuracy}')

from sklearn.metrics import f1_score
predictions = model.predict(test_generator)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = test_generator.classes
f1 = f1_score(true_classes, predicted_classes, average='weighted')
print(f'F1-Score: {f1}')

from sklearn.metrics import confusion_matrix
predictions = model.predict(test_generator)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = test_generator.classes
confusion = confusion_matrix(true_classes, predicted_classes)
print('Matriz de Confusão:')
print(confusion)